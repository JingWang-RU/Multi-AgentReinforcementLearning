{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf50f6a6-d247-4517-bc2d-acaa14a6771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutli-asset manager\n",
    "\n",
    "# initialization , at time step 0\n",
    "for i in range(M):\n",
    "    x[0][i] = S[0] .* h[0][i] # S_t of mid-market prices ???? not used in the update, h_t # of stock \n",
    "    b[i] =   # b is the wealth of a agent, non negative\n",
    "\n",
    "# update in the time step\n",
    "\n",
    "def rl_policy(): # reinforcement learning to decide the behavior of each agent\n",
    "    return u\n",
    "\n",
    "for t in range(T): \n",
    "    for i in range(M): # each x_i is a R^n vector, n stocks in total   \n",
    "            x[i][t] = S[t] .* h[i][i] # S[t] ?????\n",
    "            # S_t R^n mid-market prices of n stocks, # at time t\n",
    "            pi[i][t] = 1^T * x[i][i] + b[i][t] # pi is the portfolio value, B_t is the cash balance\n",
    "            \n",
    "            u[i][t] = RL_policy() # u_t is the vector of trades, depende on the policy\n",
    "            x[i][t] = x[t][i] + u[t][i] \n",
    "            \n",
    "            #pi_t_plus = pi_t + w_t - c_t - tc_t_ut # w_t receive the money flow, pay fee c_t\n",
    "            #pi_t_plus = 1^T*(x_t[i]+u_t[i]) + b_t_plus[i] # B_t^+ is the cash balance after adjusting the asset positions\n",
    "            tc[i][t] = u[i][t]^T * sigma[i][t] * u[i][t] # Sigma is the dollar, tc transaction cost\n",
    "            b_plus[i][t] = b[i][t]-1^T* u[i][t] + w[i][t] - c[i][t] - tc[i][t] \n",
    "            if b_plus[i][t] < 0:\n",
    "                        # take action\n",
    "                    \n",
    "            pi[t+1][i]=(1+r[i][t])^T *(x[i][t]+u[i][t]) + (1+r_f)*b_plus[i][t] #  r_t stochastic rate of returns  # trading decision a_t\n",
    "\n",
    "            def f(a, hat_a, v): # internal models of price impact from other agents\n",
    "                return v * a * (hat_a - a)\n",
    "        \n",
    "            \n",
    "            bar_r[i][t] = diag(W[i][t]*Z[i][t]) + f(a[t]) # W n*K: matrix of sensitivities, Z predictors\n",
    "            def a_global_value(tau, t_prim, u):\n",
    "                return 1/tau * exp^{(t_prim-t)/tau} u[i][t]\n",
    "            \n",
    "            a[i][t] =  a_global_value(tau, t_prim, u) # a: is a global value when compute ?????\n",
    "            \n",
    "            r[i][t] - 1 r_f = bar_r[i][t] + eps[t] # r_t is the equity return, eps_t is the noise, bar_r_t is the expected returns\n",
    "           \n",
    "            # portfolio growths pi_t - (1+r_f)*pi_t \n",
    "            delta_pi[i][t] = (r[i][t] - 1^T r_f)^T * (x[i][t] + u[i][t]) + (1+r_f)*(w[i][t] - c[i][t] - tc[i][t])# portfolio growth\n",
    "\n",
    "            delta_pi[i][t]  = bar_delta_pi[i][t]+ eps[t]^T*(x[i][t] + u[i][t])# risk-free growth\n",
    "            bar_delta_pi[i][t] = bar_r[i][t]^T * (x[i][t] + u[i][t]) + (1+r_f)*(w[i][t] - c[i][t] - u[i][t] T * sigma[i][t] * u[i][t])\n",
    "            var_delta_pi[i][t] = (x[i][t] + u[i][t])^T * sigma[i][t] * (x[i][t] + u[i][t])\n",
    "            \n",
    "            # reinforcement learning for a single agent  # page 11, section 4.1 z_t now defined ????\n",
    "            bar_w[i][t] = (1+r_f)*w[i][t]\n",
    "            bar_c[i][t] = (1+r_f)*c[i][t]\n",
    "            bar_sigma[i][t] = (1+r_f)*sigma[i][t]\n",
    "            exp_delta_pi[i][t] = bar_r[i][t]*(x[i][t] + u[i][t]) + bar_w[i][t] - bar_c[i][t] - u[i][t]^T * bar_sigma[i][t] * u[i][t]\n",
    "\n",
    "            reward[i][t] = exp_delta_pi[i][t] - lamb[i] * var_delta_pi[i][t] #  y is the state\n",
    "            \n",
    "            # reward[i][t] is used in rl_policy() ? ????, where z_t is defined?\n",
    "            rl_policy(reward, ??? ) # update the policy, with other parameters\n",
    "            \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
